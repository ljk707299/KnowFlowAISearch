# `poem_server` 和 `poem_client` 测试说明

本文档详细介绍了 `poem_server.py` 和 `poem_client.py` 两个测试脚本的功能、交互模式和使用方法。

这对脚本组合演示了 `fastmcp` 框架一个非常强大和高级的功能：**客户端采样 (Client-Side Sampling)**。在这种模式下，MCP Agent (服务器) 可以将需要消耗大量计算资源的任务（例如调用大语言模型）"委托"给客户端来执行。

## 1. 核心概念：客户端采样

- **`poem_server.py` (MCP Agent)**:
  - 角色：**指令发出者 (Commander)**。
  - 它定义了两个工具：`generate_poem` (生成诗歌) 和 `summarize_document` (总结文档)。
  - 它自身**不包含**任何调用 LLM 的逻辑或 API Key。
  - 当工具被调用时，它使用 `context.sample()` 函数向客户端发送一个"采样请求"，请求客户端代为执行 LLM 调用。
  - 优点：Agent 本身可以非常轻量、安全（无需敏感的 API Key），并且可以被不同的客户端复用，每个客户端可以使用各自不同的 LLM。

- **`poem_client.py` (MCP 客户端)**:
  - 角色：**指令执行者 (Executor)**。
  - 它包含了调用大语言模型 (LLM) 所需的配置（API Key, Base URL 等）。
  - 它实现了一个名为 `my_llm_handler` 的**采样处理器 (Sampling Handler)**。
  - 当服务器发来采样请求时，`my_llm_handler` 函数会被自动调用。它负责与真实的 LLM 服务通信，并将生成的结果返回给服务器。

这种 **服务器发出指令、客户端执行计算** 的模式，是构建复杂、分布式 AI Agent 系统的一种高效方式。

## 2. 环境准备

1.  **激活虚拟环境**:
    ```bash
    source venv/bin/activate
    ```

2.  **安装依赖**:
    确保已安装 `fastmcp` 和 `openai` 等核心库。
    ```bash
    pip install -r requirements.txt
    ```

3.  **配置环境变量**:
    `poem_client.py` 需要连接到大语言模型，因此必须在项目根目录的 `.env` 文件中提供有效的 LLM 配置。
    ```ini
    # .env 文件
    ZHIPUAI_API_KEY="YOUR_API_KEY"
    ZHIPUAI_BASE_URL="https://open.bigmodel.cn/api/paas/v4/"
    MODEL_NAME="glm-4-flash"
    ```

## 3. 如何运行测试

你需要打开 **两个** 独立的终端窗口，并在每个窗口中都激活虚拟环境 (`source venv/bin/activate`)。

**终端 1: 启动 `poem_server.py` (MCP Agent)**

在第一个终端中，运行以下命令来启动 Agent 服务。它会开始监听在 `9003` 端口。

```bash
python tests/poem_server.py
```
你会看到输出：
```
正在启动古诗词与摘要 MCP Agent 服务，监听端口 9003...
```

**终端 2: 运行 `poem_client.py` (测试客户端)**

在第二个终端中，运行客户端脚本。它会连接到服务器，调用工具，并触发服务端的采样请求。

```bash
python tests/poem_client.py
```

## 4. 预期输出

当你运行 `poem_client.py` 后，你会在**客户端的终端**看到类似以下的完整交互流程输出：

```
正在连接到 MCP 服务器: http://localhost:9003/sse

==================== 测试 1: 生成诗歌 ====================
--------------------------------------------------
收到来自服务器的采样请求 (ID: ...)
  - Role: system, Content: '你是一位才华横溢的诗人。...'
  - Role: user, Content: '请创作一首关于"月光下的故乡"的简短诗歌。...'
  > LLM 返回结果: '银霜铺满地，远山入画廊。...'

生成的诗歌:
银霜铺满地，远山入画廊。
窗前明月光，低头思故乡。
清风拂柳絮，蛙鸣扰梦长。
遥寄一片心，随风到君旁。

==================== 测试 2: 总结文档 ====================
--------------------------------------------------
收到来自服务器的采样请求 (ID: ...)
  - Role: system, Content: '你是一个乐于助人的摘要生成助手。...'
  - Role: user, Content: '请总结以下文档: 在浩瀚的宇宙中，地球是目前已知唯一孕育了生命的星球。它的蓝色海洋、绿色陆地和白色云层构成了一幅美丽的画卷。然而，随着工业化的发展，环境问题日益严峻，保护地球家园已成为全人类共同的责任。...'
  > LLM 返回结果: '地球是宇宙中唯一已知的生命家园，拥有美丽的自然环境。然而，工业化带来的环境问题日益严峻，保护地球是全人类的共同责任。...'

生成的摘要:
地球是宇宙中唯一已知的生命家园，拥有美丽的自然环境。然而，工业化带来的环境问题日益严峻，保护地球是全人类的共同责任。
```

同时，在**服务器的终端**，你也会看到对应的日志：
```
接收到作诗请求，主题: 月光下的故乡
生成的诗歌: ToolResult(mimeType='text/plain', text='银霜铺满地，远山入画廊。\n窗前明月光，低头思故乡。\n清风拂柳絮，蛙鸣扰梦长。\n遥寄一片心，随风到君旁。')
接收到文档总结请求，文档内容: 在浩瀚的宇宙中，地球是目前已知唯一孕...
生成的摘要: ToolResult(mimeType='text/plain', text='地球是宇宙中唯一已知的生命家园，拥有美丽的自然环境。然而，工业化带来的环境问题日益严峻，保护地球是全人类的共同责任。')
```

这个流程完整地展示了客户端和服务端如何通过"客户端采样"模式协同工作，完成复杂的 AI 生成任务。 